{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NGRC vs Fourier vs Traditional Reservoir for INR\n",
    "\n",
    "Comparing Next Generation Reservoir Computing (NGRC) approach with Fourier features and traditional reservoir computing for Implicit Neural Representations.\n",
    "\n",
    "**Key Insight**: NGRC uses polynomial features of time-delayed inputs instead of random recurrent dynamics. For spatial INR, we adapt this to polynomial features of spatial coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations_with_replacement\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    USE_TORCH = True\n",
    "    print(\"PyTorch available for MLP training\")\n",
    "except ImportError:\n",
    "    USE_TORCH = False\n",
    "    print(\"PyTorch not available, using Ridge regression only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess image\n",
    "img = Image.open('fig/cat.png').convert('RGB')\n",
    "target_size = 256\n",
    "img = img.resize((target_size, target_size), Image.LANCZOS)\n",
    "img_array = np.array(img) / 255.0\n",
    "\n",
    "h, w, c = img_array.shape\n",
    "print(f\"Image size: {h}x{w}x{c}\")\n",
    "\n",
    "# Create coordinate grid\n",
    "coords = np.linspace(0, 1, h, endpoint=False)\n",
    "x_grid = np.stack(np.meshgrid(coords, coords), -1)  # (H, W, 2)\n",
    "\n",
    "# Train/test split (as in original Fourier paper)\n",
    "test_data = (x_grid.reshape(-1, 2), img_array.reshape(-1, 3))\n",
    "train_data = (x_grid[::2, ::2].reshape(-1, 2), img_array[::2, ::2].reshape(-1, 3))\n",
    "\n",
    "print(f\"Train: {len(train_data[0])}, Test: {len(test_data[0])}\")\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img_array)\n",
    "plt.title(f'Target Image ({h}x{w})')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: NGRC-Style Polynomial Features\n",
    "\n",
    "Following the NGRC principle: instead of random recurrence, use **explicit polynomial features**.\n",
    "\n",
    "For spatial INR with coordinates $(x, y)$:\n",
    "$$\\mathbf{O}(x,y) = [1, x, y, x^2, xy, y^2, x^3, x^2y, xy^2, y^3, \\ldots]$$\n",
    "\n",
    "This is the spatial analog of NGRC's time-delay polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrc_polynomial_features(x, degree=2):\n",
    "    \"\"\"\n",
    "    NGRC-style polynomial features for spatial coordinates.\n",
    "    \n",
    "    For degree=2: [1, x, y, x², xy, y²]\n",
    "    For degree=3: [1, x, y, x², xy, y², x³, x²y, xy², y³]\n",
    "    etc.\n",
    "    \n",
    "    This is the spatial analog of NGRC's time-delay polynomial expansion.\n",
    "    \"\"\"\n",
    "    n, d = x.shape\n",
    "    features = [np.ones((n, 1))]  # Constant term\n",
    "    \n",
    "    for deg in range(1, degree + 1):\n",
    "        # All monomials of this degree\n",
    "        for powers in combinations_with_replacement(range(d), deg):\n",
    "            # powers is like (0,), (1,), (0,0), (0,1), (1,1), etc.\n",
    "            term = np.ones(n)\n",
    "            for p in powers:\n",
    "                term = term * x[:, p]\n",
    "            features.append(term.reshape(-1, 1))\n",
    "    \n",
    "    return np.hstack(features)\n",
    "\n",
    "\n",
    "def ngrc_polynomial_features_fast(x, degree=2):\n",
    "    \"\"\"\n",
    "    Faster implementation using sklearn-style polynomial expansion.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        poly = PolynomialFeatures(degree=degree, include_bias=True)\n",
    "        return poly.fit_transform(x)\n",
    "    except ImportError:\n",
    "        return ngrc_polynomial_features(x, degree)\n",
    "\n",
    "\n",
    "# Test\n",
    "test_x = np.array([[0.5, 0.5]])\n",
    "for deg in [2, 3, 4, 5]:\n",
    "    feats = ngrc_polynomial_features_fast(test_x, deg)\n",
    "    print(f\"Degree {deg}: {feats.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Fourier Features (Original Paper)\n",
    "\n",
    "$$\\gamma(\\mathbf{v}) = [\\sin(2\\pi \\mathbf{B} \\mathbf{v}), \\cos(2\\pi \\mathbf{B} \\mathbf{v})]^T$$\n",
    "\n",
    "where $\\mathbf{B} \\sim \\mathcal{N}(0, \\sigma^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_features(x, num_features, sigma):\n",
    "    \"\"\"\n",
    "    Random Fourier features as in the original paper.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    B = np.random.randn(num_features, x.shape[1]) * sigma\n",
    "    x_proj = (2. * np.pi * x) @ B.T\n",
    "    return np.concatenate([np.sin(x_proj), np.cos(x_proj)], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Traditional Reservoir (What We Were Doing)\n",
    "\n",
    "$$\\mathbf{h}^{(l)} = \\tanh(\\mathbf{W}_{in}^{(l)} \\mathbf{h}^{(l-1)} + \\mathbf{W}_{hh}^{(l)} \\mathbf{h}^{(l)} + \\mathbf{b}^{(l)})$$\n",
    "\n",
    "Iterative settling with recurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_reservoir(x, hidden_size, num_layers=5, iterations=10, spectral_radius=0.9):\n",
    "    \"\"\"\n",
    "    Traditional deep reservoir with recurrence.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    n, d = x.shape\n",
    "    \n",
    "    layers = []\n",
    "    d_in = d\n",
    "    for layer in range(num_layers):\n",
    "        np.random.seed(42 + layer)\n",
    "        W_in = np.random.randn(d_in, hidden_size) * 0.5\n",
    "        W_hh = np.random.randn(hidden_size, hidden_size)\n",
    "        eig = np.abs(np.linalg.eigvals(W_hh)).max()\n",
    "        W_hh = W_hh * (spectral_radius / eig)\n",
    "        b = np.random.randn(hidden_size) * 0.1\n",
    "        layers.append((W_in, W_hh, b))\n",
    "        d_in = hidden_size\n",
    "    \n",
    "    all_states = []\n",
    "    for i in tqdm(range(n), desc='Reservoir', leave=False):\n",
    "        layer_input = x[i:i+1]\n",
    "        layer_states = []\n",
    "        \n",
    "        for W_in, W_hh, b in layers:\n",
    "            h = np.zeros(hidden_size)\n",
    "            for _ in range(iterations):\n",
    "                h = np.tanh(layer_input @ W_in + h @ W_hh + b).flatten()\n",
    "            layer_states.append(h)\n",
    "            layer_input = h.reshape(1, -1)\n",
    "        \n",
    "        all_states.append(np.concatenate(layer_states))\n",
    "    \n",
    "    return np.array(all_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 4: NGRC-Style Random Nonlinear Projection\n",
    "\n",
    "Recent NGRC variants use **random nonlinear projections** instead of explicit polynomials:\n",
    "$$\\mathbf{O}(x) = \\phi(\\mathbf{W} \\mathbf{x} + \\mathbf{b})$$\n",
    "\n",
    "where $\\phi$ is a nonlinearity. This is like a single-layer random network (no recurrence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_nonlinear_projection(x, num_features, nonlinearity='tanh'):\n",
    "    \"\"\"\n",
    "    NGRC-style random nonlinear projection (no recurrence).\n",
    "    Similar to Extreme Learning Machine / Random Kitchen Sink.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    W = np.random.randn(x.shape[1], num_features) * np.sqrt(2.0 / x.shape[1])\n",
    "    b = np.random.randn(num_features) * 0.1\n",
    "    \n",
    "    proj = x @ W + b\n",
    "    \n",
    "    if nonlinearity == 'tanh':\n",
    "        return np.tanh(proj)\n",
    "    elif nonlinearity == 'relu':\n",
    "        return np.maximum(0, proj)\n",
    "    elif nonlinearity == 'sin':\n",
    "        return np.sin(proj)\n",
    "    else:\n",
    "        return proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def ridge_regression(H_train, y_train, H_test, y_test, lamb=1e-6):\n    \"\"\"Ridge regression with evaluation.\"\"\"\n    W = np.linalg.solve(H_train.T @ H_train + lamb * np.eye(H_train.shape[1]), H_train.T @ y_train)\n    pred = np.clip(H_test @ W, 0, 1)\n    mse = np.mean((pred - y_test) ** 2)\n    psnr = -10 * np.log10(mse) if mse > 0 else 100\n    return pred, mse, psnr, W"
  },
  {
   "cell_type": "code",
   "source": "if USE_TORCH:\n    class MLP(nn.Module):\n        \"\"\"MLP decoder as in the original Fourier paper.\"\"\"\n        def __init__(self, input_dim, hidden_dim=256, num_layers=4):\n            super().__init__()\n            layers = []\n            d_in = input_dim\n            for i in range(num_layers - 1):\n                layers.append(nn.Linear(d_in, hidden_dim))\n                layers.append(nn.ReLU())\n                d_in = hidden_dim\n            layers.append(nn.Linear(d_in, 3))\n            layers.append(nn.Sigmoid())\n            self.net = nn.Sequential(*layers)\n        \n        def forward(self, x):\n            return self.net(x)\n    \n    def train_mlp(H_train, y_train, H_test, y_test, \n                  num_layers=4, hidden_dim=256, lr=1e-4, iters=2000):\n        \"\"\"Train MLP on features exactly as in Demo.ipynb.\"\"\"\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        print(f\"    Using device: {device}\")\n        \n        model = MLP(H_train.shape[1], hidden_dim, num_layers).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n        \n        H_train_t = torch.FloatTensor(H_train).to(device)\n        y_train_t = torch.FloatTensor(y_train).to(device)\n        H_test_t = torch.FloatTensor(H_test).to(device)\n        y_test_t = torch.FloatTensor(y_test).to(device)\n        \n        train_psnrs, test_psnrs = [], []\n        \n        pbar = tqdm(range(iters), desc='Training MLP', leave=False)\n        for i in pbar:\n            optimizer.zero_grad()\n            pred = model(H_train_t)\n            loss = 0.5 * torch.mean((pred - y_train_t) ** 2)\n            loss.backward()\n            optimizer.step()\n            \n            if i % 100 == 0:\n                with torch.no_grad():\n                    train_mse = torch.mean((model(H_train_t) - y_train_t) ** 2).item()\n                    test_mse = torch.mean((model(H_test_t) - y_test_t) ** 2).item()\n                    train_psnr = -10 * np.log10(train_mse)\n                    test_psnr = -10 * np.log10(test_mse)\n                    train_psnrs.append(train_psnr)\n                    test_psnrs.append(test_psnr)\n                    pbar.set_postfix({'train': f'{train_psnr:.1f}', 'test': f'{test_psnr:.1f}'})\n        \n        with torch.no_grad():\n            pred = model(H_test_t).cpu().numpy()\n            mse = np.mean((pred - y_test) ** 2)\n            psnr = -10 * np.log10(mse)\n        \n        return pred, mse, psnr, train_psnrs, test_psnrs, model\nelse:\n    print(\"PyTorch not available - MLP training disabled\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## MLP Decoder (as in original Fourier paper)\n\nFollowing Demo.ipynb: 4-layer MLP with ReLU activations and Sigmoid output.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: NGRC Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"NGRC-STYLE POLYNOMIAL FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for degree in [2, 3, 4, 5, 6, 7, 8, 10, 12, 15]:\n",
    "    H_train = ngrc_polynomial_features_fast(train_data[0], degree)\n",
    "    H_test = ngrc_polynomial_features_fast(test_data[0], degree)\n",
    "    \n",
    "    try:\n",
    "        pred, mse, psnr, W = ridge_regression(H_train, train_data[1], H_test, test_data[1])\n",
    "        results[f'ngrc_poly_d{degree}'] = {'pred': pred, 'mse': mse, 'psnr': psnr, 'dim': H_train.shape[1]}\n",
    "        print(f\"  Degree {degree:>2}: PSNR = {psnr:.2f} dB (dim={H_train.shape[1]})\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Degree {degree:>2}: FAILED - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Fourier Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FOURIER FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for num_feat in [128, 256, 512, 1024]:\n",
    "    for sigma in [1, 10, 100]:\n",
    "        H_train = fourier_features(train_data[0], num_feat, sigma)\n",
    "        H_test = fourier_features(test_data[0], num_feat, sigma)\n",
    "        \n",
    "        pred, mse, psnr, W = ridge_regression(H_train, train_data[1], H_test, test_data[1])\n",
    "        name = f'fourier_n{num_feat}_s{sigma}'\n",
    "        results[name] = {'pred': pred, 'mse': mse, 'psnr': psnr, 'dim': H_train.shape[1]}\n",
    "        print(f\"  n={num_feat:>4}, σ={sigma:>3}: PSNR = {psnr:.2f} dB (dim={H_train.shape[1]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Random Nonlinear Projection (NGRC variant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RANDOM NONLINEAR PROJECTION (NGRC-style, no recurrence)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for num_feat in [256, 512, 1024, 2048]:\n",
    "    for nonlin in ['tanh', 'relu', 'sin']:\n",
    "        H_train = random_nonlinear_projection(train_data[0], num_feat, nonlin)\n",
    "        H_test = random_nonlinear_projection(test_data[0], num_feat, nonlin)\n",
    "        \n",
    "        pred, mse, psnr, W = ridge_regression(H_train, train_data[1], H_test, test_data[1])\n",
    "        name = f'random_{nonlin}_n{num_feat}'\n",
    "        results[name] = {'pred': pred, 'mse': mse, 'psnr': psnr, 'dim': H_train.shape[1]}\n",
    "        print(f\"  {nonlin:>4}, n={num_feat:>4}: PSNR = {psnr:.2f} dB\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Key Insights from MLP-INR Comparison\nif mlp_results:\n    print(\"\\n\" + \"=\" * 70)\n    print(\"KEY INSIGHTS: MLP-BASED INR WITH DIFFERENT FEATURES\")\n    print(\"=\" * 70)\n    \n    # Find best in each category\n    best_none = mlp_results.get('mlp_none', {}).get('psnr', 0)\n    best_poly = max([r['psnr'] for k, r in mlp_results.items() if 'poly' in k], default=0)\n    best_fourier = max([r['psnr'] for k, r in mlp_results.items() if 'fourier' in k], default=0)\n    best_random = max([r['psnr'] for k, r in mlp_results.items() if 'random' in k], default=0)\n    \n    print(f\"\"\"\n┌─────────────────────────────────────────────────────────────────┐\n│                    MLP-INR RESULTS SUMMARY                       │\n├─────────────────────────────────────────────────────────────────┤\n│  No mapping (raw x,y):              {best_none:>6.2f} dB                  │\n│  NGRC Polynomial (best):            {best_poly:>6.2f} dB                  │\n│  Fourier Features (best):           {best_fourier:>6.2f} dB                  │\n│  Random Projection (best):          {best_random:>6.2f} dB                  │\n├─────────────────────────────────────────────────────────────────┤\n│  Fourier - Polynomial gap:          {best_fourier - best_poly:>+6.2f} dB                  │\n│  Fourier - Random gap:              {best_fourier - best_random:>+6.2f} dB                  │\n│  Polynomial - None gap:             {best_poly - best_none:>+6.2f} dB                  │\n└─────────────────────────────────────────────────────────────────┘\n\nANALYSIS:\n━━━━━━━━━\n\n1. INPUT ENCODING IS CRITICAL\n   The same MLP architecture performs very differently depending\n   on how inputs are encoded. This confirms the Fourier paper's\n   key finding: the spectral bias of MLPs can be overcome with\n   proper input encoding.\n\n2. NGRC POLYNOMIAL vs FOURIER\n   - Polynomials help, but Fourier features are better for images\n   - Why? Natural images are bandlimited (smooth frequency content)\n   - Fourier basis naturally captures this; polynomials don't\n\n3. RANDOM sin(Wx+b) ≈ FOURIER\n   Random sinusoidal projection works similarly to Fourier features!\n   This is because sin(Wx+b) is essentially sampling random frequencies.\n\n4. THE SPECTRAL BIAS PROBLEM\n   - Raw (x,y) → MLP learns low frequencies first (slow for detail)\n   - Fourier features → directly encode all frequencies\n   - Polynomials → local basis, poor for periodic/oscillatory content\n\n5. NGRC LESSON FOR INR\n   NGRC's key insight (replace recurrence with explicit features)\n   applies to INR too: the RIGHT features matter more than \n   network depth or architecture complexity.\n\"\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# MLP Visual Comparison\nif mlp_results:\n    fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n    \n    # Row 1: Reconstructions\n    axes[0, 0].imshow(img_array)\n    axes[0, 0].set_title('Original', fontsize=12)\n    axes[0, 0].axis('off')\n    \n    sorted_mlp = sorted(mlp_results.items(), key=lambda x: x[1]['psnr'], reverse=True)\n    for i, (name, r) in enumerate(sorted_mlp[:4]):\n        pred = r['pred'].reshape(h, w, 3)\n        axes[0, i+1].imshow(pred)\n        short_name = name.replace('mlp_', '').replace('ngrc_', '')\n        axes[0, i+1].set_title(f'{short_name}\\n{r[\"psnr\"]:.1f} dB', fontsize=10)\n        axes[0, i+1].axis('off')\n    \n    # Row 2: Training curves\n    ax = axes[1, 0]\n    for name, r in mlp_results.items():\n        if 'test_psnrs' in r:\n            xs = np.arange(len(r['test_psnrs'])) * 100\n            short_name = name.replace('mlp_', '').replace('ngrc_', '')\n            ax.plot(xs, r['test_psnrs'], label=short_name)\n    ax.set_xlabel('Iteration')\n    ax.set_ylabel('Test PSNR (dB)')\n    ax.set_title('Training Curves')\n    ax.legend(fontsize=8)\n    ax.grid(True, alpha=0.3)\n    \n    # Bar chart\n    ax = axes[1, 1]\n    names = [n.replace('mlp_', '').replace('ngrc_', '') for n, _ in sorted_mlp]\n    psnrs = [r['psnr'] for _, r in sorted_mlp]\n    colors = ['purple' if 'poly' in n else 'blue' if 'fourier' in n else 'orange' if 'random' in n else 'gray' for n in names]\n    ax.barh(range(len(names)), psnrs, color=colors, alpha=0.8)\n    ax.set_yticks(range(len(names)))\n    ax.set_yticklabels(names, fontsize=9)\n    ax.set_xlabel('PSNR (dB)')\n    ax.set_title('MLP-INR Comparison')\n    ax.grid(True, alpha=0.3, axis='x')\n    \n    # Hide unused\n    for i in range(2, 5):\n        axes[1, i].axis('off')\n    \n    plt.tight_layout()\n    plt.savefig('ngrc_mlp_comparison.png', dpi=150, bbox_inches='tight')\n    plt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# MLP Results Summary\nif mlp_results:\n    print(\"\\n\" + \"=\" * 70)\n    print(\"MLP-BASED INR RESULTS (sorted by PSNR)\")\n    print(\"=\" * 70)\n    \n    sorted_mlp = sorted(mlp_results.items(), key=lambda x: x[1]['psnr'], reverse=True)\n    print(f\"{'Method':<30} {'PSNR (dB)':<12} {'Input Dim':<10}\")\n    print(\"-\" * 52)\n    for name, r in sorted_mlp:\n        print(f\"{name:<30} {r['psnr']:<12.2f} {r['dim']:<10}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "mlp_results = {}\n\nif USE_TORCH:\n    print(\"=\" * 70)\n    print(\"MLP-BASED INR COMPARISON\")\n    print(\"=\" * 70)\n    print(\"Same MLP (4 layers, 256 hidden, ReLU, Sigmoid output)\")\n    print(\"Different input features\\n\")\n    \n    # 1. No mapping (raw coordinates) - baseline\n    print(\"1. RAW COORDINATES (no mapping)\")\n    H_train = train_data[0]  # Just (x, y)\n    H_test = test_data[0]\n    pred, mse, psnr, train_psnrs, test_psnrs, _ = train_mlp(\n        H_train, train_data[1], H_test, test_data[1], iters=2000\n    )\n    mlp_results['mlp_none'] = {'pred': pred, 'psnr': psnr, 'dim': 2, \n                               'train_psnrs': train_psnrs, 'test_psnrs': test_psnrs}\n    print(f\"    PSNR = {psnr:.2f} dB\\n\")\n    \n    # 2. NGRC Polynomial features\n    print(\"2. NGRC POLYNOMIAL FEATURES\")\n    for degree in [3, 5, 8]:\n        print(f\"  Degree {degree}:\")\n        H_train = ngrc_polynomial_features_fast(train_data[0], degree)\n        H_test = ngrc_polynomial_features_fast(test_data[0], degree)\n        pred, mse, psnr, train_psnrs, test_psnrs, _ = train_mlp(\n            H_train, train_data[1], H_test, test_data[1], iters=2000\n        )\n        mlp_results[f'mlp_ngrc_poly_d{degree}'] = {\n            'pred': pred, 'psnr': psnr, 'dim': H_train.shape[1],\n            'train_psnrs': train_psnrs, 'test_psnrs': test_psnrs\n        }\n        print(f\"    PSNR = {psnr:.2f} dB (dim={H_train.shape[1]})\\n\")\n    \n    # 3. Fourier features (different scales)\n    print(\"3. FOURIER FEATURES\")\n    for sigma in [1, 10, 100]:\n        print(f\"  σ = {sigma}:\")\n        H_train = fourier_features(train_data[0], 256, sigma)\n        H_test = fourier_features(test_data[0], 256, sigma)\n        pred, mse, psnr, train_psnrs, test_psnrs, _ = train_mlp(\n            H_train, train_data[1], H_test, test_data[1], iters=2000\n        )\n        mlp_results[f'mlp_fourier_s{sigma}'] = {\n            'pred': pred, 'psnr': psnr, 'dim': H_train.shape[1],\n            'train_psnrs': train_psnrs, 'test_psnrs': test_psnrs\n        }\n        print(f\"    PSNR = {psnr:.2f} dB (dim={H_train.shape[1]})\\n\")\n    \n    # 4. Random nonlinear projection\n    print(\"4. RANDOM NONLINEAR PROJECTION\")\n    for nonlin in ['tanh', 'sin']:\n        print(f\"  {nonlin}:\")\n        H_train = random_nonlinear_projection(train_data[0], 512, nonlin)\n        H_test = random_nonlinear_projection(test_data[0], 512, nonlin)\n        pred, mse, psnr, train_psnrs, test_psnrs, _ = train_mlp(\n            H_train, train_data[1], H_test, test_data[1], iters=2000\n        )\n        mlp_results[f'mlp_random_{nonlin}'] = {\n            'pred': pred, 'psnr': psnr, 'dim': H_train.shape[1],\n            'train_psnrs': train_psnrs, 'test_psnrs': test_psnrs\n        }\n        print(f\"    PSNR = {psnr:.2f} dB (dim={H_train.shape[1]})\\n\")\n\nelse:\n    print(\"PyTorch not available - skipping MLP experiments\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Experiment 5: MLP-based INR with Different Features\n\nThis is the key comparison: same MLP architecture, different input features.\nFollowing the original Fourier paper setup exactly.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Traditional Reservoir (for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRADITIONAL DEEP RESERVOIR (with recurrence)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for num_layers, hidden in [(5, 128), (10, 64)]:\n",
    "    name = f'reservoir_L{num_layers}_H{hidden}'\n",
    "    print(f\"  Computing {name}...\")\n",
    "    \n",
    "    H_train = deep_reservoir(train_data[0], hidden, num_layers=num_layers, iterations=10)\n",
    "    H_test = deep_reservoir(test_data[0], hidden, num_layers=num_layers, iterations=10)\n",
    "    \n",
    "    pred, mse, psnr, W = ridge_regression(H_train, train_data[1], H_test, test_data[1])\n",
    "    results[name] = {'pred': pred, 'mse': mse, 'psnr': psnr, 'dim': H_train.shape[1]}\n",
    "    print(f\"    PSNR = {psnr:.2f} dB (dim={H_train.shape[1]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by PSNR\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1]['psnr'], reverse=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ALL RESULTS (sorted by PSNR)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Method':<35} {'PSNR (dB)':<12} {'Dim':<8}\")\n",
    "print(\"-\" * 55)\n",
    "for name, r in sorted_results[:20]:  # Top 20\n",
    "    print(f\"{name:<35} {r['psnr']:<12.2f} {r['dim']:<8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best in each category\n",
    "categories = {\n",
    "    'NGRC Polynomial': [k for k in results if 'ngrc_poly' in k],\n",
    "    'Fourier': [k for k in results if 'fourier' in k],\n",
    "    'Random Projection': [k for k in results if 'random_' in k],\n",
    "    'Traditional Reservoir': [k for k in results if 'reservoir' in k],\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"BEST IN EACH CATEGORY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "best_results = {}\n",
    "for cat, keys in categories.items():\n",
    "    if keys:\n",
    "        best_key = max(keys, key=lambda k: results[k]['psnr'])\n",
    "        best_results[cat] = best_key\n",
    "        r = results[best_key]\n",
    "        print(f\"{cat:<25}: {best_key:<30} PSNR = {r['psnr']:.2f} dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "\n",
    "# Row 1: Best from each method\n",
    "axes[0, 0].imshow(img_array)\n",
    "axes[0, 0].set_title('Original', fontsize=12)\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "for i, (cat, key) in enumerate(best_results.items()):\n",
    "    if i < 4:\n",
    "        pred = results[key]['pred'].reshape(h, w, 3)\n",
    "        axes[0, i+1].imshow(pred)\n",
    "        axes[0, i+1].set_title(f'{cat}\\n{results[key][\"psnr\"]:.1f} dB', fontsize=10)\n",
    "        axes[0, i+1].axis('off')\n",
    "\n",
    "# Row 2: NGRC polynomial scaling\n",
    "poly_keys = sorted([k for k in results if 'ngrc_poly' in k], \n",
    "                   key=lambda k: int(k.split('_d')[1]))\n",
    "for i, key in enumerate(poly_keys[:5]):\n",
    "    pred = results[key]['pred'].reshape(h, w, 3)\n",
    "    axes[1, i].imshow(pred)\n",
    "    deg = key.split('_d')[1]\n",
    "    axes[1, i].set_title(f'Poly deg={deg}\\n{results[key][\"psnr\"]:.1f} dB', fontsize=10)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ngrc_comparison_images.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart by category\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "names = [r[0] for r in sorted_results]\n",
    "psnrs = [r[1]['psnr'] for r in sorted_results]\n",
    "\n",
    "colors = []\n",
    "for n in names:\n",
    "    if 'ngrc_poly' in n:\n",
    "        colors.append('purple')\n",
    "    elif 'fourier' in n:\n",
    "        colors.append('blue')\n",
    "    elif 'random_' in n:\n",
    "        colors.append('orange')\n",
    "    elif 'reservoir' in n:\n",
    "        colors.append('green')\n",
    "    else:\n",
    "        colors.append('gray')\n",
    "\n",
    "bars = ax.bar(range(len(names)), psnrs, color=colors, alpha=0.8)\n",
    "ax.set_xticks(range(len(names)))\n",
    "ax.set_xticklabels(names, rotation=45, ha='right', fontsize=7)\n",
    "ax.set_ylabel('PSNR (dB)', fontsize=12)\n",
    "ax.set_title('NGRC vs Fourier vs Random Projection vs Traditional Reservoir', fontsize=14)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='purple', alpha=0.8, label='NGRC Polynomial'),\n",
    "    Patch(facecolor='blue', alpha=0.8, label='Fourier'),\n",
    "    Patch(facecolor='orange', alpha=0.8, label='Random Projection'),\n",
    "    Patch(facecolor='green', alpha=0.8, label='Traditional Reservoir'),\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ngrc_comparison_barchart.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PSNR vs feature dimension for each method\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# NGRC Polynomial\n",
    "poly_data = [(results[k]['dim'], results[k]['psnr']) for k in results if 'ngrc_poly' in k]\n",
    "poly_data.sort()\n",
    "ax.plot([d[0] for d in poly_data], [d[1] for d in poly_data], 'p-', \n",
    "        color='purple', label='NGRC Polynomial', markersize=10, linewidth=2)\n",
    "\n",
    "# Fourier (best sigma)\n",
    "fourier_data = [(results[k]['dim'], results[k]['psnr']) for k in results \n",
    "                if 'fourier' in k and '_s10' in k]  # sigma=10\n",
    "fourier_data.sort()\n",
    "ax.plot([d[0] for d in fourier_data], [d[1] for d in fourier_data], 's-', \n",
    "        color='blue', label='Fourier (σ=10)', markersize=10, linewidth=2)\n",
    "\n",
    "# Random tanh\n",
    "random_data = [(results[k]['dim'], results[k]['psnr']) for k in results \n",
    "               if 'random_tanh' in k]\n",
    "random_data.sort()\n",
    "ax.plot([d[0] for d in random_data], [d[1] for d in random_data], 'o-', \n",
    "        color='orange', label='Random tanh', markersize=10, linewidth=2)\n",
    "\n",
    "# Random sin  \n",
    "random_sin = [(results[k]['dim'], results[k]['psnr']) for k in results \n",
    "              if 'random_sin' in k]\n",
    "random_sin.sort()\n",
    "ax.plot([d[0] for d in random_sin], [d[1] for d in random_sin], '^-', \n",
    "        color='red', label='Random sin', markersize=10, linewidth=2)\n",
    "\n",
    "# Traditional reservoir\n",
    "res_data = [(results[k]['dim'], results[k]['psnr']) for k in results if 'reservoir' in k]\n",
    "for d, p in res_data:\n",
    "    ax.scatter([d], [p], color='green', s=150, marker='*', zorder=5)\n",
    "ax.scatter([], [], color='green', s=150, marker='*', label='Traditional Reservoir')\n",
    "\n",
    "ax.set_xlabel('Feature Dimension', fontsize=12)\n",
    "ax.set_ylabel('PSNR (dB)', fontsize=12)\n",
    "ax.set_title('PSNR vs Feature Dimension', fontsize=14)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ngrc_scaling.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY INSIGHTS: NGRC vs TRADITIONAL APPROACHES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "1. NGRC PRINCIPLE APPLIED TO INR\n",
    "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "   NGRC replaces recurrent dynamics with explicit polynomial features.\n",
    "   For spatial INR: polynomial features of (x,y) coordinates.\n",
    "   \n",
    "   This is essentially POLYNOMIAL REGRESSION on coordinates.\n",
    "\n",
    "2. WHY FOURIER STILL WINS\n",
    "   ━━━━━━━━━━━━━━━━━━━━━━\n",
    "   - Natural images are BANDLIMITED (smooth, not polynomial)\n",
    "   - Fourier basis matches image statistics\n",
    "   - Polynomials diverge at boundaries, oscillate badly\n",
    "   - sin/cos are globally smooth and periodic\n",
    "\n",
    "3. WHAT NGRC TEACHES US\n",
    "   ━━━━━━━━━━━━━━━━━━━━\n",
    "   - Recurrence is NOT necessary for good features\n",
    "   - The right BASIS FUNCTION matters more than architecture\n",
    "   - For temporal data: time-delay polynomials work\n",
    "   - For spatial data: Fourier (frequency) basis works\n",
    "\n",
    "4. RANDOM sin(Wx+b) ≈ FOURIER\n",
    "   ━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "   Random sinusoidal projection approximates Fourier features!\n",
    "   This connects NGRC random projection to Fourier methods.\n",
    "\n",
    "5. RESERVOIR'S REAL VALUE\n",
    "   ━━━━━━━━━━━━━━━━━━━━━━\n",
    "   - NOT in basis function quality\n",
    "   - IN temporal memory and dynamics\n",
    "   - For static INR: use Fourier or NGRC polynomial\n",
    "   - For temporal: use ESN or NGRC time-delay features\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Hybrid NGRC + Fourier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"HYBRID: NGRC POLYNOMIAL + FOURIER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Combine polynomial and Fourier features\n",
    "for poly_deg in [3, 5]:\n",
    "    for fourier_n in [256, 512]:\n",
    "        H_poly_train = ngrc_polynomial_features_fast(train_data[0], poly_deg)\n",
    "        H_poly_test = ngrc_polynomial_features_fast(test_data[0], poly_deg)\n",
    "        \n",
    "        H_fourier_train = fourier_features(train_data[0], fourier_n, sigma=10)\n",
    "        H_fourier_test = fourier_features(test_data[0], fourier_n, sigma=10)\n",
    "        \n",
    "        H_train = np.hstack([H_poly_train, H_fourier_train])\n",
    "        H_test = np.hstack([H_poly_test, H_fourier_test])\n",
    "        \n",
    "        pred, mse, psnr, W = ridge_regression(H_train, train_data[1], H_test, test_data[1])\n",
    "        name = f'hybrid_poly{poly_deg}_fourier{fourier_n}'\n",
    "        results[name] = {'pred': pred, 'mse': mse, 'psnr': psnr, 'dim': H_train.shape[1]}\n",
    "        print(f\"  Poly d={poly_deg} + Fourier n={fourier_n}: PSNR = {psnr:.2f} dB (dim={H_train.shape[1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL RANKING (Top 10)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sorted_all = sorted(results.items(), key=lambda x: x[1]['psnr'], reverse=True)\n",
    "print(f\"{'Rank':<6} {'Method':<40} {'PSNR (dB)':<12} {'Dim':<8}\")\n",
    "print(\"-\" * 66)\n",
    "for i, (name, r) in enumerate(sorted_all[:10]):\n",
    "    print(f\"{i+1:<6} {name:<40} {r['psnr']:<12.2f} {r['dim']:<8}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}