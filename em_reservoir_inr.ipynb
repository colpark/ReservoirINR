{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM-Style Reservoir Training for INR\n",
    "\n",
    "**Key Idea**: Alternate between training the readout and updating the reservoir.\n",
    "\n",
    "Traditional RC: Random fixed reservoir → train only readout\n",
    "\n",
    "EM-RC: Iterate until convergence:\n",
    "- **E-step**: Fix reservoir weights → solve for optimal readout (ridge regression)\n",
    "- **M-step**: Fix readout → update reservoir weights to reduce reconstruction error\n",
    "\n",
    "This simulates an **evolving reservoir** that adapts when improvement stalls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    USE_TORCH = True\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"PyTorch available, using {device}\")\n",
    "except ImportError:\n",
    "    USE_TORCH = False\n",
    "    print(\"PyTorch not available, using NumPy only\")\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = Image.open('fig/cat.png').convert('RGB')\n",
    "target_size = 128\n",
    "img = img.resize((target_size, target_size), Image.LANCZOS)\n",
    "img_array = np.array(img) / 255.0\n",
    "\n",
    "h, w, c = img_array.shape\n",
    "coords = np.linspace(0, 1, h, endpoint=False)\n",
    "x_grid = np.stack(np.meshgrid(coords, coords), -1)\n",
    "X = x_grid.reshape(-1, 2)\n",
    "Y = img_array.reshape(-1, 3)\n",
    "\n",
    "print(f\"Image: {h}x{w}, Samples: {len(X)}\")\n",
    "plt.imshow(img_array)\n",
    "plt.title('Target')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM-Reservoir: NumPy Implementation\n",
    "\n",
    "```\n",
    "Initialize: Random W_in, W_hh, b\n",
    "\n",
    "Repeat until convergence:\n",
    "    E-step: h = reservoir(X; W_in, W_hh, b)\n",
    "            W_out = ridge_solve(h, Y)  # Optimal readout\n",
    "            \n",
    "    M-step: Compute gradient of loss w.r.t. W_in, W_hh, b\n",
    "            Update reservoir weights via gradient descent\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMReservoir:\n",
    "    \"\"\"\n",
    "    EM-style reservoir where both reservoir and readout are trained alternately.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_size, output_dim, \n",
    "                 num_layers=1, spectral_radius=0.9):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.spectral_radius = spectral_radius\n",
    "        \n",
    "        # Initialize reservoir weights\n",
    "        self._init_reservoir()\n",
    "        \n",
    "        # Readout weights (will be computed in E-step)\n",
    "        self.W_out = None\n",
    "        \n",
    "    def _init_reservoir(self):\n",
    "        \"\"\"Initialize reservoir weights.\"\"\"\n",
    "        self.layers = []\n",
    "        d_in = self.input_dim\n",
    "        \n",
    "        for l in range(self.num_layers):\n",
    "            W_in = np.random.randn(d_in, self.hidden_size) * 0.5\n",
    "            W_hh = np.random.randn(self.hidden_size, self.hidden_size)\n",
    "            # Normalize spectral radius\n",
    "            eig = np.abs(np.linalg.eigvals(W_hh)).max()\n",
    "            W_hh = W_hh * (self.spectral_radius / eig)\n",
    "            b = np.random.randn(self.hidden_size) * 0.1\n",
    "            \n",
    "            self.layers.append({'W_in': W_in, 'W_hh': W_hh, 'b': b})\n",
    "            d_in = self.hidden_size\n",
    "    \n",
    "    def forward(self, X, num_iterations=10):\n",
    "        \"\"\"Forward pass through reservoir.\"\"\"\n",
    "        n = X.shape[0]\n",
    "        all_h = []\n",
    "        \n",
    "        layer_input = X\n",
    "        for layer in self.layers:\n",
    "            W_in, W_hh, b = layer['W_in'], layer['W_hh'], layer['b']\n",
    "            h = np.zeros((n, self.hidden_size))\n",
    "            \n",
    "            # Iterative settling\n",
    "            for _ in range(num_iterations):\n",
    "                h = np.tanh(layer_input @ W_in + h @ W_hh + b)\n",
    "            \n",
    "            all_h.append(h)\n",
    "            layer_input = h\n",
    "        \n",
    "        # Concatenate all layer outputs\n",
    "        H = np.hstack(all_h)\n",
    "        return H\n",
    "    \n",
    "    def e_step(self, X, Y, lamb=1e-6):\n",
    "        \"\"\"\n",
    "        E-step: Fix reservoir, compute optimal readout via ridge regression.\n",
    "        \"\"\"\n",
    "        H = self.forward(X)\n",
    "        # Ridge regression: W_out = (H^T H + λI)^(-1) H^T Y\n",
    "        self.W_out = np.linalg.solve(\n",
    "            H.T @ H + lamb * np.eye(H.shape[1]), \n",
    "            H.T @ Y\n",
    "        )\n",
    "        return H\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict output.\"\"\"\n",
    "        H = self.forward(X)\n",
    "        return np.clip(H @ self.W_out, 0, 1)\n",
    "    \n",
    "    def compute_loss(self, X, Y):\n",
    "        \"\"\"Compute MSE loss.\"\"\"\n",
    "        pred = self.predict(X)\n",
    "        mse = np.mean((pred - Y) ** 2)\n",
    "        psnr = -10 * np.log10(mse) if mse > 0 else 100\n",
    "        return mse, psnr\n",
    "\n",
    "# Test\n",
    "em_res = EMReservoir(input_dim=2, hidden_size=256, output_dim=3, num_layers=3)\n",
    "H = em_res.e_step(X, Y)\n",
    "mse, psnr = em_res.compute_loss(X, Y)\n",
    "print(f\"Initial (random reservoir): PSNR = {psnr:.2f} dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M-Step: Update Reservoir Weights\n",
    "\n",
    "Use gradient descent to update reservoir weights while keeping readout fixed.\n",
    "\n",
    "We need gradients through the reservoir, so we'll use PyTorch for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if USE_TORCH:\n    class EMReservoirTorch(nn.Module):\n        \"\"\"\n        EM-style reservoir with trainable weights.\n        \"\"\"\n        def __init__(self, input_dim, hidden_size, output_dim,\n                     num_layers=1, spectral_radius=0.9, num_iterations=10):\n            super().__init__()\n            self.hidden_size = hidden_size\n            self.num_layers = num_layers\n            self.num_iterations = num_iterations\n            self.spectral_radius = spectral_radius\n\n            # Reservoir weights (trainable in M-step)\n            self.W_in = nn.ParameterList()\n            self.W_hh = nn.ParameterList()\n            self.b = nn.ParameterList()\n\n            d_in = input_dim\n            for l in range(num_layers):\n                self.W_in.append(nn.Parameter(torch.randn(d_in, hidden_size) * 0.5))\n\n                # Initialize W_hh with spectral radius control\n                W_hh_init = torch.randn(hidden_size, hidden_size)\n                eig = torch.linalg.eigvals(W_hh_init).abs().max()\n                W_hh_init = W_hh_init * (spectral_radius / eig)\n                self.W_hh.append(nn.Parameter(W_hh_init))\n\n                self.b.append(nn.Parameter(torch.randn(hidden_size) * 0.1))\n                d_in = hidden_size\n\n            # Readout weights (computed in E-step, not trained by gradient)\n            self.register_buffer('W_out', torch.zeros(num_layers * hidden_size, output_dim))\n\n        def forward(self, X):\n            \"\"\"Forward pass through reservoir.\"\"\"\n            batch_size = X.shape[0]\n            all_h = []\n\n            layer_input = X\n            for l in range(self.num_layers):\n                h = torch.zeros(batch_size, self.hidden_size, device=X.device)\n\n                for _ in range(self.num_iterations):\n                    h = torch.tanh(layer_input @ self.W_in[l] + h @ self.W_hh[l] + self.b[l])\n\n                all_h.append(h)\n                layer_input = h\n\n            H = torch.cat(all_h, dim=1)\n            return H\n\n        def e_step(self, X, Y, lamb=1e-3):\n            \"\"\"\n            E-step: Compute optimal readout (closed-form ridge regression).\n            Uses lstsq for numerical stability.\n            \"\"\"\n            with torch.no_grad():\n                H = self.forward(X)\n                # Add regularization directly to H for stability\n                n, d = H.shape\n                H_reg = torch.cat([H, torch.sqrt(torch.tensor(lamb, device=H.device)) * torch.eye(d, device=H.device)], dim=0)\n                Y_reg = torch.cat([Y, torch.zeros(d, Y.shape[1], device=Y.device)], dim=0)\n                # Use lstsq (more stable than solve)\n                result = torch.linalg.lstsq(H_reg, Y_reg)\n                self.W_out = result.solution\n            return H\n\n        def predict(self, X):\n            \"\"\"Predict with current reservoir and readout.\"\"\"\n            H = self.forward(X)\n            return torch.clamp(H @ self.W_out, 0, 1)\n\n        def m_step_loss(self, X, Y):\n            \"\"\"\n            M-step loss: reconstruction error with FIXED readout.\n            Gradients flow through reservoir weights only.\n            \"\"\"\n            H = self.forward(X)\n            # Use detached W_out (no gradient through it)\n            pred = torch.clamp(H @ self.W_out.detach(), 0, 1)\n            return torch.mean((pred - Y) ** 2)\n\n    print(\"EMReservoirTorch defined\")\nelse:\n    print(\"PyTorch not available\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if USE_TORCH:\n    def train_em_reservoir(X, Y, hidden_size=256, num_layers=3,\n                           num_em_iterations=20, m_steps_per_em=100,\n                           lr=1e-3, patience=3):\n        \"\"\"\n        EM-style training:\n        1. E-step: Fix reservoir, solve for optimal readout\n        2. M-step: Fix readout, update reservoir via gradient descent\n        3. Repeat until convergence\n        \"\"\"\n        X_t = torch.FloatTensor(X).to(device)\n        Y_t = torch.FloatTensor(Y).to(device)\n\n        model = EMReservoirTorch(\n            input_dim=2, hidden_size=hidden_size, output_dim=3,\n            num_layers=num_layers, num_iterations=10\n        ).to(device)\n\n        # Optimizer for reservoir weights only\n        reservoir_params = list(model.W_in) + list(model.W_hh) + list(model.b)\n        optimizer = torch.optim.Adam(reservoir_params, lr=lr)\n\n        history = {'em_iter': [], 'psnr': [], 'mse': []}\n        best_psnr = 0\n        no_improve_count = 0\n\n        print(\"EM Training:\")\n        print(\"=\" * 60)\n\n        for em_iter in range(num_em_iterations):\n            # ============ E-STEP ============\n            model.e_step(X_t, Y_t)\n\n            with torch.no_grad():\n                pred = model.predict(X_t)\n                mse = torch.mean((pred - Y_t) ** 2).item()\n                psnr = -10 * np.log10(mse) if mse > 0 else 100\n\n            print(f\"EM Iter {em_iter+1:2d} | After E-step: PSNR = {psnr:.2f} dB\")\n\n            history['em_iter'].append(em_iter + 0.5)  # Mark E-step\n            history['psnr'].append(psnr)\n            history['mse'].append(mse)\n\n            # ============ M-STEP ============\n            # Update reservoir weights to reduce reconstruction error\n            for m_step in range(m_steps_per_em):\n                optimizer.zero_grad()\n                loss = model.m_step_loss(X_t, Y_t)\n                loss.backward()\n                # Gradient clipping for stability\n                torch.nn.utils.clip_grad_norm_(reservoir_params, max_norm=1.0)\n                optimizer.step()\n\n            with torch.no_grad():\n                pred = model.predict(X_t)\n                mse = torch.mean((pred - Y_t) ** 2).item()\n                psnr_after_m = -10 * np.log10(mse) if mse > 0 else 100\n\n            print(f\"           | After M-step: PSNR = {psnr_after_m:.2f} dB (Δ={psnr_after_m-psnr:+.2f})\")\n\n            history['em_iter'].append(em_iter + 1)\n            history['psnr'].append(psnr_after_m)\n            history['mse'].append(mse)\n\n            # Early stopping\n            if psnr_after_m > best_psnr + 0.1:\n                best_psnr = psnr_after_m\n                no_improve_count = 0\n            else:\n                no_improve_count += 1\n\n            if no_improve_count >= patience:\n                print(f\"\\nConverged after {em_iter+1} EM iterations\")\n                break\n\n        return model, history\n\n    print(\"Training function defined\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_TORCH:\n",
    "    # Train EM-Reservoir\n",
    "    model, history = train_em_reservoir(\n",
    "        X, Y, \n",
    "        hidden_size=256, \n",
    "        num_layers=3,\n",
    "        num_em_iterations=20,\n",
    "        m_steps_per_em=200,\n",
    "        lr=1e-3,\n",
    "        patience=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_TORCH:\n",
    "    # Visualize training progress\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # PSNR over EM iterations\n",
    "    ax = axes[0]\n",
    "    ax.plot(history['em_iter'], history['psnr'], 'bo-', markersize=6)\n",
    "    ax.set_xlabel('EM Iteration')\n",
    "    ax.set_ylabel('PSNR (dB)')\n",
    "    ax.set_title('EM Training Progress')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mark E-steps and M-steps\n",
    "    for i, (x, p) in enumerate(zip(history['em_iter'], history['psnr'])):\n",
    "        if x == int(x):  # M-step result\n",
    "            ax.annotate('M', (x, p), textcoords=\"offset points\", xytext=(0,10), fontsize=8)\n",
    "        else:  # E-step result\n",
    "            ax.annotate('E', (x, p), textcoords=\"offset points\", xytext=(0,10), fontsize=8)\n",
    "    \n",
    "    # Final image\n",
    "    ax = axes[1]\n",
    "    with torch.no_grad():\n",
    "        X_t = torch.FloatTensor(X).to(device)\n",
    "        pred = model.predict(X_t).cpu().numpy()\n",
    "    ax.imshow(pred.reshape(h, w, 3))\n",
    "    final_psnr = history['psnr'][-1]\n",
    "    ax.set_title(f'EM-Reservoir Result\\nPSNR = {final_psnr:.2f} dB')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('em_reservoir_training.png', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Random vs EM-Trained Reservoir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with random reservoir (standard approach)\n",
    "def random_reservoir_baseline(X, Y, hidden_size=256, num_layers=3):\n",
    "    \"\"\"Standard random reservoir with ridge regression.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    n, d = X.shape\n",
    "    \n",
    "    all_h = []\n",
    "    layer_input = X\n",
    "    \n",
    "    for l in range(num_layers):\n",
    "        d_in = layer_input.shape[1]\n",
    "        W_in = np.random.randn(d_in, hidden_size) * 0.5\n",
    "        W_hh = np.random.randn(hidden_size, hidden_size)\n",
    "        eig = np.abs(np.linalg.eigvals(W_hh)).max()\n",
    "        W_hh = W_hh * (0.9 / eig)\n",
    "        b = np.random.randn(hidden_size) * 0.1\n",
    "        \n",
    "        h = np.zeros((n, hidden_size))\n",
    "        for _ in range(10):\n",
    "            h = np.tanh(layer_input @ W_in + h @ W_hh + b)\n",
    "        \n",
    "        all_h.append(h)\n",
    "        layer_input = h\n",
    "    \n",
    "    H = np.hstack(all_h)\n",
    "    W_out = np.linalg.solve(H.T @ H + 1e-6 * np.eye(H.shape[1]), H.T @ Y)\n",
    "    pred = np.clip(H @ W_out, 0, 1)\n",
    "    \n",
    "    mse = np.mean((pred - Y) ** 2)\n",
    "    psnr = -10 * np.log10(mse)\n",
    "    return pred, psnr\n",
    "\n",
    "# Fourier baseline\n",
    "def fourier_baseline(X, Y, num_features=256, sigma=10):\n",
    "    np.random.seed(42)\n",
    "    B = np.random.randn(num_features, 2) * sigma\n",
    "    H = np.concatenate([np.sin(2*np.pi*X@B.T), np.cos(2*np.pi*X@B.T)], axis=1)\n",
    "    W = np.linalg.solve(H.T @ H + 1e-6 * np.eye(H.shape[1]), H.T @ Y)\n",
    "    pred = np.clip(H @ W, 0, 1)\n",
    "    mse = np.mean((pred - Y) ** 2)\n",
    "    psnr = -10 * np.log10(mse)\n",
    "    return pred, psnr\n",
    "\n",
    "# Run baselines\n",
    "pred_random, psnr_random = random_reservoir_baseline(X, Y, hidden_size=256, num_layers=3)\n",
    "pred_fourier, psnr_fourier = fourier_baseline(X, Y, num_features=256, sigma=10)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Random Reservoir (fixed):    {psnr_random:.2f} dB\")\n",
    "print(f\"EM-Trained Reservoir:        {history['psnr'][-1]:.2f} dB\")\n",
    "print(f\"Fourier Features:            {psnr_fourier:.2f} dB\")\n",
    "print(f\"\\nImprovement (EM vs Random):  {history['psnr'][-1] - psnr_random:+.2f} dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "axes[0].imshow(img_array)\n",
    "axes[0].set_title('Target', fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(pred_random.reshape(h, w, 3))\n",
    "axes[1].set_title(f'Random Reservoir\\n{psnr_random:.2f} dB', fontsize=12)\n",
    "axes[1].axis('off')\n",
    "\n",
    "if USE_TORCH:\n",
    "    with torch.no_grad():\n",
    "        X_t = torch.FloatTensor(X).to(device)\n",
    "        pred_em = model.predict(X_t).cpu().numpy()\n",
    "    axes[2].imshow(pred_em.reshape(h, w, 3))\n",
    "    axes[2].set_title(f'EM-Trained Reservoir\\n{history[\"psnr\"][-1]:.2f} dB', fontsize=12)\n",
    "else:\n",
    "    axes[2].set_title('PyTorch required')\n",
    "axes[2].axis('off')\n",
    "\n",
    "axes[3].imshow(pred_fourier.reshape(h, w, 3))\n",
    "axes[3].set_title(f'Fourier Features\\n{psnr_fourier:.2f} dB', fontsize=12)\n",
    "axes[3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('em_reservoir_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: What Does EM Training Learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_TORCH:\n",
    "    # Analyze learned vs random reservoir weights\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # Compare W_hh spectral properties\n",
    "    for l in range(min(3, model.num_layers)):\n",
    "        # Learned W_hh\n",
    "        W_hh_learned = model.W_hh[l].detach().cpu().numpy()\n",
    "        eigs_learned = np.linalg.eigvals(W_hh_learned)\n",
    "        \n",
    "        # Random W_hh (for comparison)\n",
    "        np.random.seed(42 + l)\n",
    "        W_hh_random = np.random.randn(256, 256)\n",
    "        eig_max = np.abs(np.linalg.eigvals(W_hh_random)).max()\n",
    "        W_hh_random = W_hh_random * (0.9 / eig_max)\n",
    "        eigs_random = np.linalg.eigvals(W_hh_random)\n",
    "        \n",
    "        ax = axes[0, l]\n",
    "        ax.scatter(eigs_random.real, eigs_random.imag, alpha=0.5, s=10, label='Random')\n",
    "        ax.scatter(eigs_learned.real, eigs_learned.imag, alpha=0.5, s=10, label='Learned')\n",
    "        circle = plt.Circle((0, 0), 1, fill=False, color='r', linestyle='--')\n",
    "        ax.add_patch(circle)\n",
    "        ax.set_xlim(-1.5, 1.5)\n",
    "        ax.set_ylim(-1.5, 1.5)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_title(f'Layer {l+1}: W_hh Eigenvalues')\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Weight distribution comparison\n",
    "    for l in range(min(3, model.num_layers)):\n",
    "        W_in_learned = model.W_in[l].detach().cpu().numpy().flatten()\n",
    "        \n",
    "        ax = axes[1, l]\n",
    "        ax.hist(W_in_learned, bins=50, alpha=0.7, density=True, label='Learned W_in')\n",
    "        \n",
    "        # Random for comparison\n",
    "        np.random.seed(42 + l)\n",
    "        W_in_random = (np.random.randn(2 if l == 0 else 256, 256) * 0.5).flatten()\n",
    "        ax.hist(W_in_random, bins=50, alpha=0.5, density=True, label='Random W_in')\n",
    "        \n",
    "        ax.set_title(f'Layer {l+1}: W_in Distribution')\n",
    "        ax.legend(fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('em_weight_analysis.png', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "══════════════════════════════════════════════════════════════════════\n",
    "                    KEY INSIGHTS: EM-TRAINED RESERVOIR\n",
    "══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "1. EM ALGORITHM FOR RESERVOIR\n",
    "   ━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "   E-step: Fix reservoir → optimal readout (closed-form ridge)\n",
    "   M-step: Fix readout → update reservoir (gradient descent)\n",
    "   \n",
    "   This decomposes the joint optimization into tractable subproblems.\n",
    "\n",
    "2. EVOLVING RESERVOIR\n",
    "   ━━━━━━━━━━━━━━━━━━\n",
    "   Traditional RC: Random fixed reservoir (never adapts)\n",
    "   EM-RC: Reservoir evolves to better represent the data\n",
    "   \n",
    "   The reservoir learns to generate features that are more\n",
    "   linearly separable for the readout.\n",
    "\n",
    "3. WHEN DOES EM HELP?\n",
    "   ━━━━━━━━━━━━━━━━━━\n",
    "   ✓ When random initialization is suboptimal\n",
    "   ✓ When the task requires specific feature structure\n",
    "   ✓ When you can afford the extra computation\n",
    "   \n",
    "   ✗ May not help if random reservoir is already good\n",
    "   ✗ More expensive than standard RC\n",
    "\n",
    "4. CONNECTION TO OTHER METHODS\n",
    "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "   - Alternating minimization\n",
    "   - Coordinate descent\n",
    "   - Two-stage training (pretrain encoder, train decoder)\n",
    "   - Self-training / iterative refinement\n",
    "\n",
    "5. SPECTRAL CHANGES\n",
    "   ━━━━━━━━━━━━━━━━━\n",
    "   EM training can change the spectral properties of W_hh.\n",
    "   The learned reservoir may have different dynamics than\n",
    "   the random initialization.\n",
    "\n",
    "══════════════════════════════════════════════════════════════════════\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}